# File: malware_detection_text.py
# Adapted from EfficientNetB0.py for text-based malware detection using merged_positive.csv and negatives from merged.csv

import os
import time
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, f1_score, recall_score, precision_score
from sklearn.model_selection import train_test_split

# Tat cac log khong can thiet cua TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# --- PHAN 1: CAU HINH VA CAC HAM HO TRO ---

# --- Cau hinh cho mo hinh ---
CONFIG = {
    "MODEL_NAME": "MalwareDetection_Text_LSTM",
    "MAX_TOKENS": 10000,  # Vocab size for text
    "SEQUENCE_LENGTH": 200,  # Max length of text sequences
    "EMBEDDING_DIM": 128,
    "BATCH_SIZE": 32,
    "EPOCHS": 10,
    "OUTPUT_DIR": 'output'
}

def get_device_info():
    """Kiem tra va tra ve thong tin ve thiet bi (GPU/CPU)."""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            gpu_info = tf.config.experimental.get_device_details(gpus[0])
            gpu_name = gpu_info.get('device_name', 'GPU')
            print(f"Da tim thay {len(gpus)} GPU. Se su dung: {gpu_name}")
            return gpu_name
        except RuntimeError as e:
            print(e)
            return "GPU (Loi khi khoi tao)"
    else:
        print("Khong tim thay GPU. Se su dung CPU (qua trinh co the rat cham).")
        return "CPU"

def load_and_prepare_data():
    """Tai du lieu tu CSV, chuan bi text va labels. Tach DDoS rieng."""
    print("\nDang tai du lieu...")

    # Load from individual datasets to separate DDoS
    datasets = {
        'XSS': r'..\dataset\XSS_dataset.csv',
        'SQL': r'..\dataset\Modified_SQL_Dataset.csv',
        'DDOS': r'..\dataset\DDOS_dataset.csv'
    }

    df_list = []
    for source, path in datasets.items():
        if os.path.exists(path):
            df = pd.read_csv(path)
            df['source'] = source  # Add source column
            df_list.append(df)
            print(f"Loaded {len(df)} samples from {source}")
        else:
            print(f"Warning: {path} not found")

    if not df_list:
        print("No datasets loaded")
        return None, None, None, None

    df_all = pd.concat(df_list, ignore_index=True)

    # Separate DDoS for separate training/testing
    df_ddos = df_all[df_all['source'] == 'DDOS'].copy()
    df_non_ddos = df_all[df_all['source'] != 'DDOS'].copy()

    print(f"DDoS samples: {len(df_ddos)}")
    print(f"Non-DDoS samples: {len(df_non_ddos)}")

    # Use non-DDoS for main training (XSS + SQL positives and negatives)
    df = df_non_ddos.copy()

    # Filter out empty or very short texts (at least 3 words)
    df = df[df['Sentence'].str.strip().str.split().str.len() > 2]
    print(f"Total samples after filtering: {len(df)} (Pos: {len(df[df['Label']==1])}, Neg: {len(df[df['Label']==0])})")

    # Assume 'Sentence' column for text
    texts = df['Sentence'].fillna('').astype(str).values
    labels = df['Label'].values

    # Split into train/val/test (80-10-10)
    train_texts, temp_texts, train_labels, temp_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels)
    val_texts, test_texts, val_labels, test_labels = train_test_split(
        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)

    print(f"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}")

    # Text vectorization
    vectorize_layer = keras.layers.TextVectorization(
        max_tokens=CONFIG["MAX_TOKENS"],
        output_mode='int',
        output_sequence_length=CONFIG["SEQUENCE_LENGTH"]
    )
    vectorize_layer.adapt(train_texts)

    # Create tf.data datasets
    def vectorize_text(text, label):
        return vectorize_layer(text), label

    AUTOTUNE = tf.data.AUTOTUNE

    train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))
    train_ds = train_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    train_ds = train_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    val_ds = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))
    val_ds = val_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    val_ds = val_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    test_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels))
    test_ds = test_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    test_ds = test_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    return train_ds, val_ds, test_ds, vectorize_layer

def build_text_model(vocab_size, embedding_dim):
    """
    Xay dung mo hinh LSTM cho text classification.
    """
    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim),
        keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),
        keras.layers.Bidirectional(keras.layers.LSTM(32)),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid')  # Binary classification
    ])
    return model

def plot_and_save_history(history, save_path):
    """Ve va luu bieu do ket qua huan luyen."""
    pd.DataFrame(history.history).plot(figsize=(10, 6))
    plt.grid(True)
    plt.title("Ket qua Huan luyen")
    plt.xlabel("Epoch")
    plt.savefig(save_path)
    plt.close()
    print(f"Da luu bieu do ket qua huan luyen tai: {save_path}")

def plot_confusion_matrix(y_true, y_pred, save_path):
    """Ve va luu confusion matrix."""
    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.savefig(save_path)
    plt.close()
    print(f"Da luu confusion matrix tai: {save_path}")

def plot_roc_curve(y_true, y_pred_probs, save_path):
    """Ve va luu ROC curve."""
    from sklearn.metrics import roc_curve, auc
    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.savefig(save_path)
    plt.close()
    print(f"Da luu ROC curve tai: {save_path}")

# --- PHAN 2: QUY TRINH CHINH ---

def main():
    os.makedirs(CONFIG["OUTPUT_DIR"], exist_ok=True)

    print("--- BAT DAU GIAI DOAN HUAN LUYEN ---")

    device_name = get_device_info()

    train_ds, val_ds, test_ds, vectorize_layer = load_and_prepare_data()
    if train_ds is None:
        return

    model = build_text_model(CONFIG["MAX_TOKENS"], CONFIG["EMBEDDING_DIM"])

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',  # Binary
        metrics=['accuracy']
    )

    print("\nBat dau huan luyen mo hinh...")
    start_time_train = time.time()

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=CONFIG["EPOCHS"]
    )

    end_time_train = time.time()
    training_time_s = end_time_train - start_time_train
    training_time_m = training_time_s / 60
    print(
        f"Huan luyen hoan tat trong {training_time_s:.2f} giay ({training_time_m:.2f} phut).")

    plot_path = os.path.join(
        CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_training_history.png')
    plot_and_save_history(history, plot_path)

    model_path = os.path.join(
        CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}.keras')
    model.save(model_path)
    print(f"Da luu mo hinh da huan luyen tai: {model_path}")

    print("\n--- BAT DAU GIAI DOAN KIEM TRA (TEST) ---")

    print("Dang tai lai mo hinh da luu de kiem tra...")
    loaded_model = keras.models.load_model(model_path)

    # Get true labels
    y_true = []
    for _, labels in test_ds:
        y_true.extend(labels.numpy())
    y_true = np.array(y_true)

    print("Dang thuc hien du doan tren tap test...")
    start_time_pred = time.time()
    y_pred_probs = loaded_model.predict(test_ds)
    end_time_pred = time.time()

    y_pred = (y_pred_probs > 0.5).astype(int).flatten()

    num_test_samples = len(y_true)
    total_pred_time = end_time_pred - start_time_pred
    time_per_sample = total_pred_time / num_test_samples

    accuracy = np.sum(y_pred == y_true) / num_test_samples
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)

    model_size_bytes = os.path.getsize(model_path)
    model_size_mb = model_size_bytes / (1024 * 1024)

    execution_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    results = {
        "thuat_toan": CONFIG["MODEL_NAME"],
        "GPU": device_name,
        "thoi_gian_huan_luyen (s)": round(training_time_s, 2),
        "thoi_gian_huan_luyen (m)": round(training_time_m, 2),
        "accuracy": f"{accuracy:.4f}",
        "F1_score": f"{f1:.4f}",
        "recall": f"{recall:.4f}",
        "precision": f"{precision:.4f}",
        "thoi_gian_du_doan_1_mau (s)": f"{time_per_sample:.6f}",
        "model_size (MB)": round(model_size_mb, 2),
        "ngay_thuc_hien": execution_date,
    }

    print("\n--- KET QUA DANH GIA TREN TAP TEST ---")
    for key, value in results.items():
        print(f"{key.replace('_', ' ').capitalize():<30}: {value}")
    print("-----------------------------------------")
    print("\nChi tiet theo tung lop:")
    print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive'], zero_division=0))

    # Plot additional metrics
    cm_path = os.path.join(CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_confusion_matrix.png')
    plot_confusion_matrix(y_true, y_pred, cm_path)

    roc_path = os.path.join(CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_roc_curve.png')
    plot_roc_curve(y_true, y_pred_probs.flatten(), roc_path)

    results_df = pd.DataFrame([results])
    csv_path = os.path.join(CONFIG["OUTPUT_DIR"], 'evaluation_results.csv')

    if os.path.exists(csv_path):
        results_df.to_csv(csv_path, mode='a', header=False, index=False)
        print(f"Da cap nhat ket qua vao file: {csv_path}")
    else:
        results_df.to_csv(csv_path, mode='w', header=True, index=False)
        print(f"Da luu ket qua vao file moi: {csv_path}")

    print("\n--- TOAN BO QUA TRINH HOAN TAT ---")

if __name__ == '__main__':
    main()