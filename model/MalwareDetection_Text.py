# File: malware_detection_text.py
# Adapted from EfficientNetB0.py for text-based malware detection using merged_positive.csv and negatives from merged.csv

import os
import time
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, roc_curve, auc
from sklearn.model_selection import train_test_split

# Tat cac log khong can thiet cua TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# --- PHAN 1: CAU HINH VA CAC HAM HO TRO ---

# --- Cau hinh cho mo hinh ---
CONFIG = {
    "MODEL_NAME": "MalwareDetection_Text_LSTM",
    "MAX_TOKENS": 10000,  # Vocab size for text
    "SEQUENCE_LENGTH": 200,  # Max length of text sequences
    "EMBEDDING_DIM": 128,
    "BATCH_SIZE": 128,  # TƒÉng t·ª´ 32 l√™n 128
    "EPOCHS": 30,       # TƒÉng t·ª´ 10 l√™n 30
    "OUTPUT_DIR": 'output'
}

def get_device_info():
    """Kiem tra va tra ve thong tin ve thiet bi (GPU/CPU)."""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            gpu_info = tf.config.experimental.get_device_details(gpus[0])
            gpu_name = gpu_info.get('device_name', 'GPU')
            print(f"Da tim thay {len(gpus)} GPU. Se su dung: {gpu_name}")
            return gpu_name
        except RuntimeError as e:
            print(e)
            return "GPU (Loi khi khoi tao)"
    else:
        print("Khong tim thay GPU. Se su dung CPU (qua trinh co the rat cham).")
        return "CPU"

def load_and_prepare_data():
    """Tai du lieu tu CSV, chuan bi text va labels. Tach DDoS rieng."""
    print("\nDang tai du lieu...")

    # Load from individual datasets to separate DDoS
    datasets = {
        'XSS': 'dataset/XSS_dataset.csv',
        'SQL': 'dataset/Modified_SQL_Dataset.csv',
        'DDOS': 'dataset/DDOS_dataset.csv'
    }

    df_list = []
    for source, path in datasets.items():
        if os.path.exists(path):
            df = pd.read_csv(path)
            df['source'] = source  # Add source column
            df_list.append(df)
            print(f"Loaded {len(df)} samples from {source}")
        else:
            print(f"Warning: {path} not found")

    if not df_list:
        print("No datasets loaded")
        return None, None, None, None

    df_all = pd.concat(df_list, ignore_index=True)

    # Separate DDoS for separate training/testing
    df_ddos = df_all[df_all['source'] == 'DDOS'].copy()
    df_non_ddos = df_all[df_all['source'] != 'DDOS'].copy()

    print(f"DDoS samples: {len(df_ddos)}")
    print(f"Non-DDoS samples: {len(df_non_ddos)}")

    # Use non-DDoS for main training (XSS + SQL positives and negatives)
    df = df_non_ddos.copy()

    # Filter out empty or very short texts (at least 3 words)
    df = df[df['Sentence'].str.strip().str.split().str.len() > 2]
    print(f"Total samples after filtering: {len(df)} (Pos: {len(df[df['Label']==1])}, Neg: {len(df[df['Label']==0])})")

    # Assume 'Sentence' column for text
    texts = df['Sentence'].fillna('').astype(str).values
    labels = df['Label'].values

    # Split into train/val/test (70-15-15)
    train_texts, temp_texts, train_labels, temp_labels = train_test_split(
        texts, labels, test_size=0.3, random_state=42, stratify=labels)  # 70% train, 30% temp
    val_texts, test_texts, val_labels, test_labels = train_test_split(
        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)  # 15% val, 15% test

    print(f"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}")

    # Text vectorization
    vectorize_layer = keras.layers.TextVectorization(
        max_tokens=CONFIG["MAX_TOKENS"],
        output_mode='int',
        output_sequence_length=CONFIG["SEQUENCE_LENGTH"]
    )
    vectorize_layer.adapt(train_texts)

    # Create tf.data datasets
    def vectorize_text(text, label):
        return vectorize_layer(text), label

    AUTOTUNE = tf.data.AUTOTUNE

    train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))
    train_ds = train_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    train_ds = train_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    val_ds = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))
    val_ds = val_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    val_ds = val_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    test_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels))
    test_ds = test_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    test_ds = test_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    return train_ds, val_ds, test_ds, vectorize_layer

def build_text_model(vocab_size, embedding_dim):
    """
    Xay dung mo hinh LSTM cho text classification.
    """
    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim),
        keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),
        keras.layers.Bidirectional(keras.layers.LSTM(32)),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid')  # Binary classification
    ])
    return model

def plot_and_save_history(history, save_path):
    """Ve va luu bieu do chi tiet ket qua huan luyen."""
    # Tao figure voi 2 subplots
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # Plot 1: Training va Validation Accuracy
    ax1.plot(history.history['accuracy'], 'b-', linewidth=2, label='Training Accuracy')
    ax1.plot(history.history['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')
    ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Plot 2: Training va Validation Loss
    ax2.plot(history.history['loss'], 'b-', linewidth=2, label='Training Loss')
    ax2.plot(history.history['val_loss'], 'r-', linewidth=2, label='Validation Loss')
    ax2.set_title('Model Loss', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Plot 3: Accuracy vs Loss (Training)
    ax3.scatter(history.history['loss'], history.history['accuracy'],
                c=range(len(history.history['loss'])), cmap='Blues', s=50, alpha=0.7)
    ax3.set_title('Training: Accuracy vs Loss', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Loss')
    ax3.set_ylabel('Accuracy')
    ax3.grid(True, alpha=0.3)

    # Plot 4: Accuracy vs Loss (Validation)
    ax4.scatter(history.history['val_loss'], history.history['val_accuracy'],
                c=range(len(history.history['val_loss'])), cmap='Reds', s=50, alpha=0.7)
    ax4.set_title('Validation: Accuracy vs Loss', fontsize=14, fontweight='bold')
    ax4.set_xlabel('Loss')
    ax4.set_ylabel('Accuracy')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    # In thong tin tong ket
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    final_train_loss = history.history['loss'][-1]
    final_val_loss = history.history['val_loss'][-1]

    print("\nüìä TRAINING SUMMARY:")
    print(".4f")
    print(".4f")
    print(".4f")
    print(".4f")
    print(f"üìà Total Epochs: {len(history.history['accuracy'])}")
    print(f"üíæ Saved to: {save_path}")

def plot_train_test_comparison(history, y_true, y_pred_probs, save_path):
    """T·∫°o s∆° ƒë·ªì so s√°nh training v√† test performance."""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Training curves
    epochs = range(1, len(history.history['accuracy']) + 1)

    ax1.plot(epochs, history.history['accuracy'], 'b-', linewidth=2, label='Train Accuracy')
    ax1.plot(epochs, history.history['val_accuracy'], 'r-', linewidth=2, label='Val Accuracy')
    ax1.fill_between(epochs,
                     [x - 0.01 for x in history.history['accuracy']],
                     [x + 0.01 for x in history.history['accuracy']],
                     alpha=0.1, color='blue')
    ax1.fill_between(epochs,
                     [x - 0.01 for x in history.history['val_accuracy']],
                     [x + 0.01 for x in history.history['val_accuracy']],
                     alpha=0.1, color='red')
    ax1.set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Loss curves
    ax2.plot(epochs, history.history['loss'], 'b-', linewidth=2, label='Train Loss')
    ax2.plot(epochs, history.history['val_loss'], 'r-', linewidth=2, label='Val Loss')
    ax2.fill_between(epochs,
                     [x - 0.01 for x in history.history['loss']],
                     [x + 0.01 for x in history.history['loss']],
                     alpha=0.1, color='blue')
    ax2.fill_between(epochs,
                     [x - 0.01 for x in history.history['val_loss']],
                     [x + 0.01 for x in history.history['val_loss']],
                     alpha=0.1, color='red')
    ax2.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. Test ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)
    roc_auc = auc(fpr, tpr)

    ax3.plot(fpr, tpr, color='darkorange', linewidth=3, label=f'ROC curve (AUC = {roc_auc:.3f})')
    ax3.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', label='Random classifier')
    ax3.fill_between(fpr, tpr, alpha=0.2, color='darkorange')
    ax3.set_xlim([0.0, 1.0])
    ax3.set_ylim([0.0, 1.05])
    ax3.set_xlabel('False Positive Rate')
    ax3.set_ylabel('True Positive Rate')
    ax3.set_title('Test Set ROC Curve', fontsize=14, fontweight='bold')
    ax3.legend(loc="lower right")
    ax3.grid(True, alpha=0.3)

    # 4. Learning curves with overfitting detection
    ax4.plot(epochs, history.history['accuracy'], 'b-', linewidth=2, label='Train Acc')
    ax4.plot(epochs, history.history['val_accuracy'], 'r-', linewidth=2, label='Val Acc')
    ax4.axhline(y=max(history.history['val_accuracy']), color='green', linestyle='--',
                linewidth=2, label=f'Best Val Acc: {max(history.history["val_accuracy"]):.3f}')

    # Detect overfitting
    train_acc_final = history.history['accuracy'][-1]
    val_acc_final = history.history['val_accuracy'][-1]
    overfitting_gap = train_acc_final - val_acc_final

    if overfitting_gap > 0.1:
        ax4.text(0.02, 0.98, f'‚ö†Ô∏è Overfitting Detected\nGap: {overfitting_gap:.3f}',
                transform=ax4.transAxes, fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
    elif overfitting_gap < -0.05:
        ax4.text(0.02, 0.98, f'‚ö†Ô∏è Underfitting Detected\nGap: {overfitting_gap:.3f}',
                transform=ax4.transAxes, fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='orange', alpha=0.8))
    else:
        ax4.text(0.02, 0.98, f'‚úÖ Good Fit\nGap: {overfitting_gap:.3f}',
                transform=ax4.transAxes, fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

    ax4.set_title('Learning Curves & Overfitting Analysis', fontsize=14, fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Accuracy')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.suptitle(f'Malware Detection BiLSTM - Training & Test Analysis\nBatch Size: {CONFIG["BATCH_SIZE"]}, Epochs: {CONFIG["EPOCHS"]}',
                fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"üìä Train/Test comparison plot saved to: {save_path}")
    print(f"üîç Final overfitting gap: {overfitting_gap:.4f}")
    print(f"üéØ Best validation accuracy: {max(history.history['val_accuracy']):.4f} at epoch {history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1}")

def plot_train_test_comparison_simple(y_true, y_pred_probs, save_path):
    """T·∫°o s∆° ƒë·ªì so s√°nh test performance v·ªõi th√¥ng tin c∆° b·∫£n."""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Prediction Distribution
    y_pred = (y_pred_probs > 0.5).astype(int)

    ax1.hist(y_pred_probs[y_true == 0], bins=50, alpha=0.7, label='Benign', color='blue', density=True)
    ax1.hist(y_pred_probs[y_true == 1], bins=50, alpha=0.7, label='Malware', color='red', density=True)
    ax1.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')
    ax1.set_title('Prediction Probability Distribution', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Prediction Probability')
    ax1.set_ylabel('Density')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)
    roc_auc = auc(fpr, tpr)

    ax2.plot(fpr, tpr, color='darkorange', linewidth=3, label=f'ROC curve (AUC = {roc_auc:.3f})')
    ax2.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', label='Random classifier')
    ax2.fill_between(fpr, tpr, alpha=0.2, color='darkorange')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.set_title('ROC Curve Analysis', fontsize=14, fontweight='bold')
    ax2.legend(loc="lower right")
    ax2.grid(True, alpha=0.3)

    # 3. Precision-Recall Curve
    from sklearn.metrics import precision_recall_curve, average_precision_score
    precision, recall, _ = precision_recall_curve(y_true, y_pred_probs)
    avg_precision = average_precision_score(y_true, y_pred_probs)

    ax3.plot(recall, precision, color='purple', linewidth=3, label=f'PR curve (AP = {avg_precision:.3f})')
    ax3.set_xlabel('Recall')
    ax3.set_ylabel('Precision')
    ax3.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. Confusion Matrix Heatmap
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_true, y_pred)
    im = ax4.imshow(cm, interpolation='nearest', cmap='Blues')
    ax4.set_title('Confusion Matrix', fontsize=14, fontweight='bold')

    # Add text annotations
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax4.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black",
                    fontsize=12, fontweight='bold')

    ax4.set_xticks([0, 1])
    ax4.set_yticks([0, 1])
    ax4.set_xticklabels(['Predicted Benign', 'Predicted Malware'])
    ax4.set_yticklabels(['Actual Benign', 'Actual Malware'])
    ax4.set_xlabel('Predicted Label')
    ax4.set_ylabel('True Label')

    # Add colorbar
    plt.colorbar(im, ax=ax4, shrink=0.8)

    plt.suptitle(f'Malware Detection BiLSTM - Test Performance Analysis\nBatch Size: {CONFIG["BATCH_SIZE"]}, Epochs: {CONFIG["EPOCHS"]}',
                fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"üìä Test performance analysis plot saved to: {save_path}")
    print(f"üéØ Test AUC: {roc_auc:.4f}")
    print(f"üìà Test Average Precision: {avg_precision:.4f}")

def plot_confusion_matrix(y_true, y_pred, save_path):
    """Ve va luu confusion matrix."""
    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.savefig(save_path)
    plt.close()
    print(f"Da luu confusion matrix tai: {save_path}")

def plot_roc_curve(y_true, y_pred_probs, save_path):
    """Ve va luu ROC curve."""
    from sklearn.metrics import roc_curve, auc
    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.savefig(save_path)
    plt.close()
    print(f"Da luu ROC curve tai: {save_path}")

# --- PHAN 2: QUY TRINH CHINH ---

def main():
    os.makedirs(CONFIG["OUTPUT_DIR"], exist_ok=True)

    print("--- BAT DAU GIAI DOAN HUAN LUYEN ---")

    device_name = get_device_info()

    train_ds, val_ds, test_ds, vectorize_layer = load_and_prepare_data()
    if train_ds is None:
        return

    model = build_text_model(CONFIG["MAX_TOKENS"], CONFIG["EMBEDDING_DIM"])

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',  # Binary
        metrics=['accuracy']
    )

    print("\nBat dau huan luyen mo hinh...")
    start_time_train = time.time()

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=CONFIG["EPOCHS"]
    )

    end_time_train = time.time()
    training_time_s = end_time_train - start_time_train
    training_time_m = training_time_s / 60
    print(
        f"Huan luyen hoan tat trong {training_time_s:.2f} giay ({training_time_m:.2f} phut).")

    plot_path = os.path.join(
        CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_training_history.png')
    plot_and_save_history(history, plot_path)

    model_path = os.path.join(
        CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}.keras')
    model.save(model_path)
    print(f"Da luu mo hinh da huan luyen tai: {model_path}")

    print("\n--- BAT DAU GIAI DOAN KIEM TRA (TEST) ---")

    print("Dang tai lai mo hinh da luu de kiem tra...")
    loaded_model = keras.models.load_model(model_path)

    # Get true labels
    y_true = []
    for _, labels in test_ds:
        y_true.extend(labels.numpy())
    y_true = np.array(y_true)

    print("Dang thuc hien du doan tren tap test...")
    start_time_pred = time.time()
    y_pred_probs = loaded_model.predict(test_ds)
    end_time_pred = time.time()

    y_pred = (y_pred_probs > 0.5).astype(int).flatten()

    num_test_samples = len(y_true)
    total_pred_time = end_time_pred - start_time_pred
    time_per_sample = total_pred_time / num_test_samples

    accuracy = np.sum(y_pred == y_true) / num_test_samples
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)

    model_size_bytes = os.path.getsize(model_path)
    model_size_mb = model_size_bytes / (1024 * 1024)

    execution_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    results = {
        "thuat_toan": CONFIG["MODEL_NAME"],
        "GPU": device_name,
        "thoi_gian_huan_luyen (s)": round(training_time_s, 2),
        "thoi_gian_huan_luyen (m)": round(training_time_m, 2),
        "accuracy": f"{accuracy:.4f}",
        "F1_score": f"{f1:.4f}",
        "recall": f"{recall:.4f}",
        "precision": f"{precision:.4f}",
        "thoi_gian_du_doan_1_mau (s)": f"{time_per_sample:.6f}",
        "model_size (MB)": round(model_size_mb, 2),
        "ngay_thuc_hien": execution_date,
    }

    print("\n--- KET QUA DANH GIA TREN TAP TEST ---")
    for key, value in results.items():
        print(f"{key.replace('_', ' ').capitalize():<30}: {value}")
    print("-----------------------------------------")
    print("\nChi tiet theo tung lop:")
    print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive'], zero_division=0))

    # Plot additional metrics
    cm_path = os.path.join(CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_confusion_matrix.png')
    plot_confusion_matrix(y_true, y_pred, cm_path)

    roc_path = os.path.join(CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_roc_curve.png')
    plot_roc_curve(y_true, y_pred_probs.flatten(), roc_path)

    # Create comprehensive train/test comparison plot
    # Note: Since history is not saved, we'll create a simplified version
    comparison_path = os.path.join(CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_train_test_comparison.png')
    plot_train_test_comparison_simple(y_true, y_pred_probs.flatten(), comparison_path)

    results_df = pd.DataFrame([results])
    csv_path = os.path.join(CONFIG["OUTPUT_DIR"], 'evaluation_results.csv')

    if os.path.exists(csv_path):
        results_df.to_csv(csv_path, mode='a', header=False, index=False)
        print(f"Da cap nhat ket qua vao file: {csv_path}")
    else:
        results_df.to_csv(csv_path, mode='w', header=True, index=False)
        print(f"Da luu ket qua vao file moi: {csv_path}")

    print("\n--- TOAN BO QUA TRINH HOAN TAT ---")

if __name__ == '__main__':
    main()