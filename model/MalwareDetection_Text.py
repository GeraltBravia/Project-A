# File: malware_detection_text.py
# Adapted from EfficientNetB0.py for text-based malware detection using merged_positive.csv and negatives from merged.csv

import os
import time
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, f1_score, recall_score, precision_score
from sklearn.model_selection import train_test_split

# Tat cac log khong can thiet cua TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# --- PHAN 1: CAU HINH VA CAC HAM HO TRO ---

# --- Cau hinh cho mo hinh ---
CONFIG = {
    "MODEL_NAME": "MalwareDetection_Text_LSTM",
    "MAX_TOKENS": 10000,  # Vocab size for text
    "SEQUENCE_LENGTH": 200,  # Max length of text sequences
    "EMBEDDING_DIM": 128,
    "BATCH_SIZE": 32,
    "EPOCHS": 10,
    "MERGED_CSV": r'..\dataset\merged.csv',  # Path to merged.csv for negatives
    "POSITIVE_CSV": r'..\dataset\merged_positive.csv',  # Path to positives
    "OUTPUT_DIR": 'output'
}

def get_device_info():
    """Kiem tra va tra ve thong tin ve thiet bi (GPU/CPU)."""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            gpu_info = tf.config.experimental.get_device_details(gpus[0])
            gpu_name = gpu_info.get('device_name', 'GPU')
            print(f"Da tim thay {len(gpus)} GPU. Se su dung: {gpu_name}")
            return gpu_name
        except RuntimeError as e:
            print(e)
            return "GPU (Loi khi khoi tao)"
    else:
        print("Khong tim thay GPU. Se su dung CPU (qua trinh co the rat cham).")
        return "CPU"

def load_and_prepare_data():
    """Tai du lieu tu CSV, chuan bi text va labels."""
    print("\nDang tai du lieu...")

    # Load positives
    if os.path.exists(CONFIG["POSITIVE_CSV"]):
        df_pos = pd.read_csv(CONFIG["POSITIVE_CSV"])
        df_pos['Label'] = 1  # Ensure Label=1
        print(f"Loaded {len(df_pos)} positive samples from {CONFIG['POSITIVE_CSV']}")
    else:
        print(f"Positive CSV not found: {CONFIG['POSITIVE_CSV']}")
        return None, None, None, None

    # Load merged.csv for negatives (Label=0)
    if os.path.exists(CONFIG["MERGED_CSV"]):
        df_all = pd.read_csv(CONFIG["MERGED_CSV"])
        df_neg = df_all[df_all['Label'] == 0].copy()
        print(f"Loaded {len(df_neg)} negative samples from {CONFIG['MERGED_CSV']}")
    else:
        print(f"Merged CSV not found: {CONFIG['MERGED_CSV']}")
        return None, None, None, None

    # Combine
    df = pd.concat([df_pos, df_neg], ignore_index=True)
    # Filter out empty or very short texts (at least 3 words)
    df = df[df['Sentence'].str.strip().str.split().str.len() > 2]
    print(f"Total samples after filtering: {len(df)} (Pos: {len(df[df['Label']==1])}, Neg: {len(df[df['Label']==0])})")

    # Assume 'Sentence' column for text
    texts = df['Sentence'].fillna('').astype(str).values
    labels = df['Label'].values

    # Split into train/val/test
    train_texts, temp_texts, train_labels, temp_labels = train_test_split(
        texts, labels, test_size=0.3, random_state=42, stratify=labels)
    val_texts, test_texts, val_labels, test_labels = train_test_split(
        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)

    print(f"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}")

    # Text vectorization
    vectorize_layer = keras.layers.TextVectorization(
        max_tokens=CONFIG["MAX_TOKENS"],
        output_mode='int',
        output_sequence_length=CONFIG["SEQUENCE_LENGTH"]
    )
    vectorize_layer.adapt(train_texts)

    # Create tf.data datasets
    def vectorize_text(text, label):
        return vectorize_layer(text), label

    AUTOTUNE = tf.data.AUTOTUNE

    train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))
    train_ds = train_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    train_ds = train_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    val_ds = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))
    val_ds = val_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    val_ds = val_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    test_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels))
    test_ds = test_ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)
    test_ds = test_ds.batch(CONFIG["BATCH_SIZE"]).prefetch(AUTOTUNE)

    return train_ds, val_ds, test_ds, vectorize_layer

def build_text_model(vocab_size, embedding_dim):
    """
    Xay dung mo hinh LSTM cho text classification.
    """
    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim),
        keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),
        keras.layers.Bidirectional(keras.layers.LSTM(32)),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1, activation='sigmoid')  # Binary classification
    ])
    return model

def plot_and_save_history(history, save_path):
    """Ve va luu bieu do ket qua huan luyen."""
    pd.DataFrame(history.history).plot(figsize=(10, 6))
    plt.grid(True)
    plt.title("Ket qua Huan luyen")
    plt.xlabel("Epoch")
    plt.savefig(save_path)
    plt.close()
    print(f"Da luu bieu do ket qua huan luyen tai: {save_path}")

# --- PHAN 2: QUY TRINH CHINH ---

def main():
    os.makedirs(CONFIG["OUTPUT_DIR"], exist_ok=True)

    print("--- BAT DAU GIAI DOAN HUAN LUYEN ---")

    device_name = get_device_info()

    train_ds, val_ds, test_ds, vectorize_layer = load_and_prepare_data()
    if train_ds is None:
        return

    model = build_text_model(CONFIG["MAX_TOKENS"], CONFIG["EMBEDDING_DIM"])

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',  # Binary
        metrics=['accuracy']
    )

    print("\nBat dau huan luyen mo hinh...")
    start_time_train = time.time()

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=CONFIG["EPOCHS"]
    )

    end_time_train = time.time()
    training_time_s = end_time_train - start_time_train
    training_time_m = training_time_s / 60
    print(
        f"Huan luyen hoan tat trong {training_time_s:.2f} giay ({training_time_m:.2f} phut).")

    plot_path = os.path.join(
        CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}_training_history.png')
    plot_and_save_history(history, plot_path)

    model_path = os.path.join(
        CONFIG["OUTPUT_DIR"], f'{CONFIG["MODEL_NAME"]}.keras')
    model.save(model_path)
    print(f"Da luu mo hinh da huan luyen tai: {model_path}")

    print("\n--- BAT DAU GIAI DOAN KIEM TRA (TEST) ---")

    print("Dang tai lai mo hinh da luu de kiem tra...")
    loaded_model = keras.models.load_model(model_path)

    # Get true labels
    y_true = []
    for _, labels in test_ds:
        y_true.extend(labels.numpy())
    y_true = np.array(y_true)

    print("Dang thuc hien du doan tren tap test...")
    start_time_pred = time.time()
    y_pred_probs = loaded_model.predict(test_ds)
    end_time_pred = time.time()

    y_pred = (y_pred_probs > 0.5).astype(int).flatten()

    num_test_samples = len(y_true)
    total_pred_time = end_time_pred - start_time_pred
    time_per_sample = total_pred_time / num_test_samples

    accuracy = np.sum(y_pred == y_true) / num_test_samples
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)

    model_size_bytes = os.path.getsize(model_path)
    model_size_mb = model_size_bytes / (1024 * 1024)

    execution_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    results = {
        "thuat_toan": CONFIG["MODEL_NAME"],
        "GPU": device_name,
        "thoi_gian_huan_luyen (s)": round(training_time_s, 2),
        "thoi_gian_huan_luyen (m)": round(training_time_m, 2),
        "accuracy": f"{accuracy:.4f}",
        "F1_score": f"{f1:.4f}",
        "recall": f"{recall:.4f}",
        "precision": f"{precision:.4f}",
        "thoi_gian_du_doan_1_mau (s)": f"{time_per_sample:.6f}",
        "model_size (MB)": round(model_size_mb, 2),
        "ngay_thuc_hien": execution_date,
    }

    print("\n--- KET QUA DANH GIA TREN TAP TEST ---")
    for key, value in results.items():
        print(f"{key.replace('_', ' ').capitalize():<30}: {value}")
    print("-----------------------------------------")
    print("\nChi tiet theo tung lop:")
    print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive'], zero_division=0))

    results_df = pd.DataFrame([results])
    csv_path = os.path.join(CONFIG["OUTPUT_DIR"], 'evaluation_results.csv')

    if os.path.exists(csv_path):
        results_df.to_csv(csv_path, mode='a', header=False, index=False)
        print(f"Da cap nhat ket qua vao file: {csv_path}")
    else:
        results_df.to_csv(csv_path, mode='w', header=True, index=False)
        print(f"Da luu ket qua vao file moi: {csv_path}")

    print("\n--- TOAN BO QUA TRINH HOAN TAT ---")

if __name__ == '__main__':
    main()